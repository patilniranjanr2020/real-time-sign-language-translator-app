{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1cc82a7f",
   "metadata": {},
   "source": [
    "<h1>Making Mapping file </h>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3bfda813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping JSON saved to selected_20_classes_mapping.json\n",
      "Total classes mapped: 20\n",
      "Total videos collected: 158\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Base directory where videos are stored\n",
    "base_dir = r'D:\\sem-project\\sem6-project\\Datasets\\sign-language-dataset-wlasl-videos\\dataset\\SL'\n",
    "\n",
    "# 20 target classes you want to consider (case sensitive)\n",
    "target_classes = [\n",
    "    \"car\",\n",
    "    \"computer\",\n",
    "    \"door\",\n",
    "    \"friend\",\n",
    "    \"hospital\",\n",
    "    \"love\",\n",
    "    \"money\",\n",
    "    \"phone\",\n",
    "    \"school\",\n",
    "    \"stop\",\n",
    "    \"train\",\n",
    "    \"water\",\n",
    "    \"work\",\n",
    "    \"write\",\n",
    "    \"family\",\n",
    "    \"dance\",\n",
    "    \"eat\",\n",
    "    \"hello\",\n",
    "    \"play\",\n",
    "    \"read\"\n",
    "]\n",
    "\n",
    "# List actual folders in base_dir\n",
    "existing_folders = set(os.listdir(base_dir))\n",
    "\n",
    "# Take intersection with target classes (only classes that exist)\n",
    "common_classes = [cls for cls in target_classes if cls in existing_folders]\n",
    "\n",
    "mapping = {}\n",
    "\n",
    "for cls in common_classes:\n",
    "    class_dir = os.path.join(base_dir, cls)\n",
    "    video_files = [os.path.join(class_dir, f) for f in os.listdir(class_dir) if f.endswith('.mp4')]\n",
    "    mapping[cls] = video_files\n",
    "\n",
    "# Save mapping to JSON file\n",
    "output_json = 'selected_20_classes_mapping.json'\n",
    "with open(output_json, 'w') as f:\n",
    "    json.dump(mapping, f, indent=4)\n",
    "\n",
    "print(f\"Mapping JSON saved to {output_json}\")\n",
    "print(f\"Total classes mapped: {len(mapping)}\")\n",
    "total_videos = sum(len(v) for v in mapping.values())\n",
    "print(f\"Total videos collected: {total_videos}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae758038",
   "metadata": {},
   "source": [
    "<h1>training the model and saving model</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e4d2f677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Train Loss: 3.0781, Train Acc: 0.0714, Val Loss: 3.0274, Val Acc: 0.0938\n",
      "Epoch [2/50], Train Loss: 2.9917, Train Acc: 0.0794, Val Loss: 3.0627, Val Acc: 0.0312\n",
      "Epoch [3/50], Train Loss: 2.9578, Train Acc: 0.0714, Val Loss: 3.0509, Val Acc: 0.0938\n",
      "Epoch [4/50], Train Loss: 2.9647, Train Acc: 0.1032, Val Loss: 3.0544, Val Acc: 0.0938\n",
      "Epoch [5/50], Train Loss: 2.9554, Train Acc: 0.1032, Val Loss: 3.0615, Val Acc: 0.0625\n",
      "Epoch [6/50], Train Loss: 2.9320, Train Acc: 0.1270, Val Loss: 3.0394, Val Acc: 0.0625\n",
      "Epoch [7/50], Train Loss: 2.9489, Train Acc: 0.1111, Val Loss: 3.0643, Val Acc: 0.0625\n",
      "Epoch [8/50], Train Loss: 2.9280, Train Acc: 0.0952, Val Loss: 3.0625, Val Acc: 0.0625\n",
      "Epoch [9/50], Train Loss: 2.8964, Train Acc: 0.1032, Val Loss: 3.0692, Val Acc: 0.0625\n",
      "Epoch [10/50], Train Loss: 2.8817, Train Acc: 0.1190, Val Loss: 3.0999, Val Acc: 0.0938\n",
      "Epoch [11/50], Train Loss: 2.9251, Train Acc: 0.0952, Val Loss: 3.0843, Val Acc: 0.0625\n",
      "Epoch [12/50], Train Loss: 2.8868, Train Acc: 0.1111, Val Loss: 3.0870, Val Acc: 0.0625\n",
      "Epoch [13/50], Train Loss: 2.9063, Train Acc: 0.0952, Val Loss: 3.0954, Val Acc: 0.0938\n",
      "Epoch [14/50], Train Loss: 2.9118, Train Acc: 0.1190, Val Loss: 3.1089, Val Acc: 0.0625\n",
      "Epoch [15/50], Train Loss: 2.9147, Train Acc: 0.0952, Val Loss: 3.0817, Val Acc: 0.0938\n",
      "Epoch [16/50], Train Loss: 2.9079, Train Acc: 0.1111, Val Loss: 3.1071, Val Acc: 0.0625\n",
      "Epoch [17/50], Train Loss: 2.9105, Train Acc: 0.1111, Val Loss: 3.0910, Val Acc: 0.0625\n",
      "Epoch [18/50], Train Loss: 2.9135, Train Acc: 0.1032, Val Loss: 3.0808, Val Acc: 0.0625\n",
      "Epoch [19/50], Train Loss: 2.8903, Train Acc: 0.0873, Val Loss: 3.0814, Val Acc: 0.0625\n",
      "Epoch [20/50], Train Loss: 2.9056, Train Acc: 0.1270, Val Loss: 3.0987, Val Acc: 0.0625\n",
      "Epoch [21/50], Train Loss: 2.8990, Train Acc: 0.1111, Val Loss: 3.1014, Val Acc: 0.0625\n",
      "Epoch [22/50], Train Loss: 2.9279, Train Acc: 0.1111, Val Loss: 3.1047, Val Acc: 0.0625\n",
      "Epoch [23/50], Train Loss: 2.9060, Train Acc: 0.1111, Val Loss: 3.0651, Val Acc: 0.0938\n",
      "Epoch [24/50], Train Loss: 2.9135, Train Acc: 0.0952, Val Loss: 3.0798, Val Acc: 0.0625\n",
      "Epoch [25/50], Train Loss: 2.8809, Train Acc: 0.0952, Val Loss: 3.0824, Val Acc: 0.0625\n",
      "Epoch [26/50], Train Loss: 2.8870, Train Acc: 0.1111, Val Loss: 3.0963, Val Acc: 0.0938\n",
      "Epoch [27/50], Train Loss: 2.8931, Train Acc: 0.0873, Val Loss: 3.0938, Val Acc: 0.0938\n",
      "Epoch [28/50], Train Loss: 2.9029, Train Acc: 0.1032, Val Loss: 3.0859, Val Acc: 0.0938\n",
      "Epoch [29/50], Train Loss: 2.8818, Train Acc: 0.1111, Val Loss: 3.0935, Val Acc: 0.0625\n",
      "Epoch [30/50], Train Loss: 2.9053, Train Acc: 0.1270, Val Loss: 3.0748, Val Acc: 0.0938\n",
      "Epoch [31/50], Train Loss: 2.8739, Train Acc: 0.1429, Val Loss: 3.1052, Val Acc: 0.0625\n",
      "Epoch [32/50], Train Loss: 2.9099, Train Acc: 0.1190, Val Loss: 3.0860, Val Acc: 0.0625\n",
      "Epoch [33/50], Train Loss: 2.8844, Train Acc: 0.1429, Val Loss: 3.0722, Val Acc: 0.0938\n",
      "Epoch [34/50], Train Loss: 2.8807, Train Acc: 0.1111, Val Loss: 3.1008, Val Acc: 0.0625\n",
      "Epoch [35/50], Train Loss: 2.8994, Train Acc: 0.1032, Val Loss: 3.0903, Val Acc: 0.0625\n",
      "Epoch [36/50], Train Loss: 2.9038, Train Acc: 0.0873, Val Loss: 3.0746, Val Acc: 0.0938\n",
      "Epoch [37/50], Train Loss: 2.8750, Train Acc: 0.1032, Val Loss: 3.0873, Val Acc: 0.0938\n",
      "Epoch [38/50], Train Loss: 2.9102, Train Acc: 0.0794, Val Loss: 3.0800, Val Acc: 0.0938\n",
      "Epoch [39/50], Train Loss: 2.8967, Train Acc: 0.1111, Val Loss: 3.0900, Val Acc: 0.0625\n",
      "Epoch [40/50], Train Loss: 2.8730, Train Acc: 0.1190, Val Loss: 3.0988, Val Acc: 0.0625\n",
      "Epoch [41/50], Train Loss: 2.8773, Train Acc: 0.1190, Val Loss: 3.1150, Val Acc: 0.0625\n",
      "Epoch [42/50], Train Loss: 2.8866, Train Acc: 0.1190, Val Loss: 3.0891, Val Acc: 0.0625\n",
      "Epoch [43/50], Train Loss: 2.8631, Train Acc: 0.0952, Val Loss: 3.0958, Val Acc: 0.0625\n",
      "Epoch [44/50], Train Loss: 2.8791, Train Acc: 0.0952, Val Loss: 3.1191, Val Acc: 0.0625\n",
      "Epoch [45/50], Train Loss: 2.8867, Train Acc: 0.0794, Val Loss: 3.1276, Val Acc: 0.0625\n",
      "Epoch [46/50], Train Loss: 2.8732, Train Acc: 0.0952, Val Loss: 3.1155, Val Acc: 0.0625\n",
      "Epoch [47/50], Train Loss: 2.8414, Train Acc: 0.0952, Val Loss: 3.1267, Val Acc: 0.0938\n",
      "Epoch [48/50], Train Loss: 2.8283, Train Acc: 0.1270, Val Loss: 3.1345, Val Acc: 0.0625\n",
      "Epoch [49/50], Train Loss: 2.8524, Train Acc: 0.1508, Val Loss: 3.1214, Val Acc: 0.0625\n",
      "Epoch [50/50], Train Loss: 2.8421, Train Acc: 0.1111, Val Loss: 3.1473, Val Acc: 0.0938\n",
      "Model weights saved to asl_lstm_model_weights.pth\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import mediapipe as mp\n",
    "import cv2\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "# Setup MediaPipe Holistic\n",
    "mp_holistic = mp.solutions.holistic.Holistic(\n",
    "    static_image_mode=False,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5,\n",
    ")\n",
    "\n",
    "\n",
    "# Dataset Class: Extract landmarks from videos on-the-fly\n",
    "class ASLDataset(Dataset):\n",
    "    def __init__(self, mapping_file, num_frames=30, transform=None):\n",
    "        with open(mapping_file, 'r') as f:\n",
    "            self.data = json.load(f)\n",
    "        self.num_frames = num_frames\n",
    "        self.transform = transform\n",
    "\n",
    "\n",
    "        self.classes = list(self.data.keys())\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.label_encoder.fit(self.classes)\n",
    "\n",
    "\n",
    "        # flatten (video_path, label) pairs for indexing\n",
    "        self.video_label_pairs = []\n",
    "        for label, videos in self.data.items():\n",
    "            for v in videos:\n",
    "                self.video_label_pairs.append((v, label))\n",
    "\n",
    "\n",
    "        self.holistic = mp_holistic\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_label_pairs)\n",
    "\n",
    "\n",
    "    def extract_landmarks(self, video_path):\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        indices = torch.linspace(0, max(length - 1, 0), self.num_frames).long().tolist()\n",
    "\n",
    "\n",
    "        landmarks_seq = []\n",
    "        frame_idx = 0\n",
    "        success, frame = cap.read()\n",
    "\n",
    "\n",
    "        while success and frame_idx <= indices[-1]:\n",
    "            if frame_idx in indices:\n",
    "                image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                results = self.holistic.process(image_rgb)\n",
    "\n",
    "\n",
    "                frame_landmarks = []\n",
    "\n",
    "\n",
    "                # Pose landmarks (33)\n",
    "                if results.pose_landmarks:\n",
    "                    for lm in results.pose_landmarks.landmark:\n",
    "                        frame_landmarks.extend([lm.x, lm.y, lm.z])\n",
    "                else:\n",
    "                    frame_landmarks.extend([0] * 33 * 3)\n",
    "\n",
    "\n",
    "                # Face landmarks (468)\n",
    "                if results.face_landmarks:\n",
    "                    for lm in results.face_landmarks.landmark:\n",
    "                        frame_landmarks.extend([lm.x, lm.y, lm.z])\n",
    "                else:\n",
    "                    frame_landmarks.extend([0] * 468 * 3)\n",
    "\n",
    "\n",
    "                # Left and Right Hand landmarks (21 each)\n",
    "                for hand_landmarks in [results.left_hand_landmarks, results.right_hand_landmarks]:\n",
    "                    if hand_landmarks:\n",
    "                        for lm in hand_landmarks.landmark:\n",
    "                            frame_landmarks.extend([lm.x, lm.y, lm.z])\n",
    "                    else:\n",
    "                        frame_landmarks.extend([0] * 21 * 3)\n",
    "\n",
    "\n",
    "                landmarks_seq.append(frame_landmarks)\n",
    "\n",
    "\n",
    "            success, frame = cap.read()\n",
    "            frame_idx += 1\n",
    "\n",
    "\n",
    "        cap.release()\n",
    "\n",
    "\n",
    "        # Padding if fewer frames extracted\n",
    "        while len(landmarks_seq) < self.num_frames:\n",
    "            landmarks_seq.append([0] * len(landmarks_seq[0]))\n",
    "\n",
    "\n",
    "        return torch.tensor(landmarks_seq, dtype=torch.float32)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video_path, label_str = self.video_label_pairs[idx]\n",
    "        landmarks_seq = self.extract_landmarks(video_path)\n",
    "        label = self.label_encoder.transform([label_str])[0]\n",
    "\n",
    "\n",
    "        if self.transform:\n",
    "            landmarks_seq = self.transform(landmarks_seq)\n",
    "\n",
    "\n",
    "        return landmarks_seq, label\n",
    "\n",
    "\n",
    "# LSTM model\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, num_classes, dropout=0.5):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch, seq_len, features]\n",
    "        out, (hn, cn) = self.lstm(x)\n",
    "        out = self.dropout(hn[-1])\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "# Parameters\n",
    "mapping_json = r'D:/sem-project/sem6-project/20_classes/selected_20_classes_mapping.json'\n",
    "batch_size = 4\n",
    "num_epochs = 50\n",
    "learning_rate = 0.001\n",
    "hidden_dim = 256\n",
    "num_layers = 2\n",
    "num_frames = 30\n",
    "\n",
    "\n",
    "# Dataset and loader\n",
    "dataset = ASLDataset(mapping_json, num_frames=num_frames)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_set, val_set = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "# Input size: number of features per frame (Pose + Face + Hands landmarks Ã— 3)\n",
    "dummy_sample, _ = dataset[0]\n",
    "input_size = dummy_sample.shape[1]\n",
    "\n",
    "\n",
    "num_classes = len(dataset.classes)\n",
    "\n",
    "\n",
    "model = LSTMClassifier(input_dim=input_size, hidden_dim=hidden_dim, num_layers=num_layers, num_classes=num_classes)\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    train_samples = 0\n",
    "\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device).long()  # <-- Fixed here\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        train_loss += loss.item() * inputs.size(0)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        train_correct += (preds == labels).sum().item()\n",
    "        train_samples += inputs.size(0)\n",
    "\n",
    "\n",
    "    train_loss /= train_samples\n",
    "    train_acc = train_correct / train_samples\n",
    "\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_samples = 0\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device).long()  # <-- Fixed here\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            val_correct += (preds == labels).sum().item()\n",
    "            val_samples += inputs.size(0)\n",
    "\n",
    "\n",
    "    val_loss /= val_samples\n",
    "    val_acc = val_correct / val_samples\n",
    "\n",
    "\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], \"\n",
    "          f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, \"\n",
    "          f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "\n",
    "# Save weights\n",
    "torch.save(model.state_dict(), 'asl_lstm_model_weights.pth')\n",
    "print(\"Model weights saved to asl_lstm_model_weights.pth\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce3735b",
   "metadata": {},
   "source": [
    "<h1>Predicting the class of testing video</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9df10dcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video playback completed.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import mediapipe as mp\n",
    "import cv2\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from IPython.display import display, clear_output\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# ---- Paths ----\n",
    "mapping_json = r'D:/sem-project/sem6-project/20_classes/selected_20_classes_mapping.json'  # mapping file\n",
    "weights_path = r'D:/sem-project/sem6-project/20_classes/asl_lstm_model_weights.pth'         # model weights\n",
    "test_clip = r\"D:\\sem-project\\sem6-project\\Datasets\\sign-language-dataset-wlasl-videos\\dataset\\SL\\door\\17326.mp4\"  # test video\n",
    "\n",
    "# ---- MediaPipe Holistic setup ----\n",
    "mp_holistic = mp.solutions.holistic.Holistic(\n",
    "    static_image_mode=False,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5,\n",
    ")\n",
    "\n",
    "# ---- Landmark extraction function ----\n",
    "def extract_landmarks_from_video(video_path, num_frames=30):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    indices = torch.linspace(0, max(length - 1, 0), num_frames).long().tolist()\n",
    "    landmarks_seq = []\n",
    "    frame_idx = 0\n",
    "    success, frame = cap.read()\n",
    "    while success and frame_idx <= indices[-1]:\n",
    "        if frame_idx in indices:\n",
    "            image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            results = mp_holistic.process(image_rgb)\n",
    "            frame_landmarks = []\n",
    "            # Pose (33)\n",
    "            if results.pose_landmarks:\n",
    "                for lm in results.pose_landmarks.landmark:\n",
    "                    frame_landmarks.extend([lm.x, lm.y, lm.z])\n",
    "            else:\n",
    "                frame_landmarks.extend([0] * 33 * 3)\n",
    "            # Face (468)\n",
    "            if results.face_landmarks:\n",
    "                for lm in results.face_landmarks.landmark:\n",
    "                    frame_landmarks.extend([lm.x, lm.y, lm.z])\n",
    "            else:\n",
    "                frame_landmarks.extend([0] * 468 * 3)\n",
    "            # Hands (21 each)\n",
    "            for hand_landmarks in [results.left_hand_landmarks, results.right_hand_landmarks]:\n",
    "                if hand_landmarks:\n",
    "                    for lm in hand_landmarks.landmark:\n",
    "                        frame_landmarks.extend([lm.x, lm.y, lm.z])\n",
    "                else:\n",
    "                    frame_landmarks.extend([0] * 21 * 3)\n",
    "            landmarks_seq.append(frame_landmarks)\n",
    "        success, frame = cap.read()\n",
    "        frame_idx += 1\n",
    "    cap.release()\n",
    "    while len(landmarks_seq) < num_frames:\n",
    "        landmarks_seq.append([0] * len(landmarks_seq[0]))\n",
    "    return torch.tensor(landmarks_seq, dtype=torch.float32)\n",
    "\n",
    "# ---- Load classes and label encoder ----\n",
    "with open(mapping_json, 'r') as f:\n",
    "    data = json.load(f)\n",
    "classes = list(data.keys())\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(classes)\n",
    "num_classes = len(classes)\n",
    "\n",
    "# ---- Model definition ----\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, num_classes, dropout=0.5):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "    def forward(self, x):\n",
    "        out, (hn, cn) = self.lstm(x)\n",
    "        out = self.dropout(hn[-1])\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "# ---- Extract landmarks and prepare tensor ----\n",
    "num_frames = 30\n",
    "landmarks_seq = extract_landmarks_from_video(test_clip, num_frames=num_frames)\n",
    "landmarks_seq = landmarks_seq.unsqueeze(0)  # batch size 1\n",
    "\n",
    "# ---- Load model ----\n",
    "input_size = landmarks_seq.shape[2]\n",
    "hidden_dim = 256\n",
    "num_layers = 2\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = LSTMClassifier(input_dim=input_size, hidden_dim=hidden_dim, num_layers=num_layers, num_classes=num_classes)\n",
    "model.load_state_dict(torch.load(weights_path, map_location=device))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# ---- Prediction ----\n",
    "with torch.no_grad():\n",
    "    landmarks_seq = landmarks_seq.to(device)\n",
    "    output = model(landmarks_seq)\n",
    "    pred_class_index = torch.argmax(output, dim=1).item()\n",
    "    pred_label = label_encoder.inverse_transform([pred_class_index])[0]\n",
    "\n",
    "print(f\"Predicted class label: {pred_label}\")\n",
    "\n",
    "# ---- Display video inline with prediction overlay ----\n",
    "cap = cv2.VideoCapture(test_clip)\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open video.\")\n",
    "else:\n",
    "    # Prepare font for overlay using PIL\n",
    "    try:\n",
    "        font = ImageFont.truetype(\"arial.ttf\", 40)\n",
    "    except IOError:\n",
    "        font = ImageFont.load_default()\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Convert BGR to RGB for correct colors\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        pil_img = Image.fromarray(rgb_frame)\n",
    "        \n",
    "        # Draw predicted label text\n",
    "        draw = ImageDraw.Draw(pil_img)\n",
    "        draw.text((30, 30), f\"Prediction: {pred_label}\", font=font, fill=(255, 0, 0))\n",
    "        \n",
    "        # Convert back to array for display\n",
    "        display_img = np.array(pil_img)\n",
    "        \n",
    "        # Display inline\n",
    "        clear_output(wait=True)\n",
    "        display(Image.fromarray(display_img))\n",
    "        \n",
    "        # Delay for approx 30 FPS\n",
    "        time.sleep(0.033)\n",
    "    \n",
    "    cap.release()\n",
    "    clear_output(wait=True)\n",
    "    print(\"Video playback completed.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tempvenv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
